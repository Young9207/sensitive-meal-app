#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
diet_analyzer.py
ì…ë ¥(ì•„ì¹¨/ì˜¤ì „ ê°„ì‹/ì ì‹¬/ì˜¤í›„ ê°„ì‹/ì €ë…) â†’ ì‹í’ˆ ë§¤ì¹­ â†’ ì˜ì–‘ ë¶„ì„ â†’ ë‹¤ìŒ ì‹ì‚¬ ì œì•ˆ â†’ log.csv ì €ì¥
+ ë§¤ì¹­ ì•ˆëœ ì¬ë£ŒëŠ” food_dbì— ì‹ ê·œ ì‹í’ˆ í–‰ìœ¼ë¡œ ì¶”ê°€í•˜ì—¬ food_db_updated.csv ìƒì„±

- í•„ìš” íŒŒì¼: food_db.csv (ì‹í’ˆ, ë“±ê¸‰, íƒœê·¸(ì˜ì–‘), íƒœê·¸ë¦¬ìŠ¤íŠ¸), nutrient_dict.csv(ì˜ì–‘ì†Œ, í•œì¤„ì„¤ëª… ...)
- ì‹¤í–‰: streamlit run diet_analyzer.py
- íŒŒì‹± ê·œì¹™:
  * ì‰¼í‘œ(,) / ì¤„ë°”ê¿ˆìœ¼ë¡œ 1ì°¨ ë¶„ë¦¬ â†’ ê° í† í°ì„ '+' ë¡œ 2ì°¨ ë¶„ë¦¬
  * í† í° ë ìˆ«ìëŠ” ìˆ˜ëŸ‰ìœ¼ë¡œ í•´ì„ (ì˜ˆ: 'ìš°ë©”ë³´ì‹œ2' â†’ ì´ë¦„='ìš°ë©”ë³´ì‹œ', ìˆ˜ëŸ‰=2.0) ì—†ìœ¼ë©´ 1.0
"""

from __future__ import annotations

import re
import sys
import ast
import json
import base64
import zlib
from collections import defaultdict
from typing import List, Dict, Tuple, Any

from datetime import datetime, date, timedelta
from zoneinfo import ZoneInfo

import pandas as pd

try:
    import streamlit as st
except Exception:
    st = None  # allow import without Streamlit


# ====================== ì„¤ì • ======================
FOOD_DB_CSV = "food_db.csv"
NUTRIENT_DICT_CSV = "nutrient_dict.csv"
LOG_CSV = "log.csv"
FOOD_DB_UPDATED_CSV = "food_db_updated.csv"

SLOTS = ["ì•„ì¹¨", "ì•„ì¹¨ë³´ì¡°ì œ", "ì˜¤ì „ ê°„ì‹", "ì ì‹¬", "ì ì‹¬ë³´ì¡°ì œ", "ì˜¤í›„ ê°„ì‹", "ì €ë…", "ì €ë…ë³´ì¡°ì œ", "ì €ë… ê°„ì‹"]


# ==================== íƒ€ì„ì¡´/í•˜ë£¨ ìƒíƒœ ====================
TZ = ZoneInfo("Europe/Paris")

def today_str() -> str:
    return datetime.now(TZ).date().isoformat()

def next_midnight():
    now = datetime.now(TZ)
    nm = (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=TZ)
    return nm

def init_daily_state():
    """ìì • ë‹¨ìœ„ë¡œ stateë¥¼ ìœ ì§€. ë‚ ì§œ ë°”ë€Œë©´ ìë™ ì´ˆê¸°í™”."""
    if "daily_date" not in st.session_state:
        st.session_state.daily_date = today_str()
    if st.session_state.daily_date != today_str():
        for k in ["inputs", "last_items_df", "last_nutri_df", "last_recs", "last_combo"]:
            st.session_state.pop(k, None)
        st.session_state.daily_date = today_str()
    # ì…ë ¥/ê²°ê³¼ ì»¨í…Œì´ë„ˆ ì¤€ë¹„
    st.session_state.setdefault("inputs", {s: "" for s in SLOTS})
    st.session_state.setdefault("last_items_df", None)
    st.session_state.setdefault("last_nutri_df", None)
    st.session_state.setdefault("last_recs", [])
    st.session_state.setdefault("last_combo", [])
    st.session_state.setdefault("threshold", 1)
    st.session_state.setdefault("export_flag", True)

def init_daily_state():
    """ìì • ë‹¨ìœ„ë¡œ stateë¥¼ ìœ ì§€. ë‚ ì§œ ë°”ë€Œë©´ ìë™ ì´ˆê¸°í™”."""
    if "daily_date" not in st.session_state:
        st.session_state.daily_date = today_str()
    if st.session_state.daily_date != today_str():
        for k in ["inputs", "last_items_df", "last_nutri_df", "last_recs", "last_combo"]:
            st.session_state.pop(k, None)
        st.session_state.daily_date = today_str()
    # ì…ë ¥/ê²°ê³¼ ì»¨í…Œì´ë„ˆ ì¤€ë¹„
    st.session_state.setdefault("inputs", {s: "" for s in SLOTS})
    st.session_state.setdefault("last_items_df", None)
    st.session_state.setdefault("last_nutri_df", None)
    st.session_state.setdefault("last_recs", [])
    st.session_state.setdefault("last_combo", [])
    st.session_state.setdefault("threshold", 1)
    st.session_state.setdefault("export_flag", True)

    # âœ… ì¶”ê°€: log.csvê°€ ìˆìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œì˜ ì…ë ¥ ìë™ ë³µì›
    try:
        df_log = pd.read_csv(LOG_CSV)
        today_logs = df_log[df_log["date"] == today_str()]
        if not today_logs.empty:
            # ìŠ¬ë¡¯ë³„ë¡œ ìµœê·¼ ì…ë ¥ ìë™ ë³µì›
            for slot in SLOTS:
                latest = today_logs[today_logs["slot"] == slot]
                if not latest.empty:
                    joined_items = ", ".join(latest["ì…ë ¥í•­ëª©"].tolist())
                    st.session_state.inputs[slot] = joined_items
            # í™”ë©´ í‘œì‹œìš© DFë„ ë³µì›
            st.session_state.last_items_df = today_logs.rename(columns={
                "slot": "ìŠ¬ë¡¯",
                "date": "ë‚ ì§œ"
            })
    except FileNotFoundError:
        pass



# ==================== ìœ í‹¸/ì „ì²˜ë¦¬ ====================
def _parse_tags_from_slash(cell) -> List[str]:
    if pd.isna(cell):
        return []
    return [t.strip() for t in str(cell).split('/') if t.strip()]

def _parse_taglist_cell(cell: Any) -> List[str]:
    """
    íƒœê·¸ë¦¬ìŠ¤íŠ¸ ì…€ì„ 'í•­ìƒ ë¦¬ìŠ¤íŠ¸'ë¡œ ë³€í™˜.
      - íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´: "['ë‹¨ë°±ì§ˆ', 'ì €ì§€ë°©']"
      - JSON ë°°ì—´ ë¬¸ìì—´: '["ë‹¨ë°±ì§ˆ","ì €ì§€ë°©"]'
      - ìŠ¬ë˜ì‹œ/ì‰¼í‘œ êµ¬ë¶„: "ë‹¨ë°±ì§ˆ/ì €ì§€ë°©" ë˜ëŠ” "ë‹¨ë°±ì§ˆ, ì €ì§€ë°©"
      - ì‹¤ì œ ë¦¬ìŠ¤íŠ¸: ['ë‹¨ë°±ì§ˆ', 'ì €ì§€ë°©']
      - ë¹ˆ ê°’/[] â†’ []
    """
    if isinstance(cell, list):
        return [str(x).strip() for x in cell if str(x).strip()]
    s = "" if cell is None or (isinstance(cell, float) and pd.isna(cell)) else str(cell).strip()
    if not s or s == "[]":
        return []
    # 1) literal_eval
    try:
        v = ast.literal_eval(s)
        if isinstance(v, list):
            return [str(x).strip() for x in v if str(x).strip()]
    except Exception:
        pass
    # 2) JSON
    try:
        v = json.loads(s)
        if isinstance(v, list):
            return [str(x).strip() for x in v if str(x).strip()]
    except Exception:
        pass
    # 3) êµ¬ë¶„ì ê¸°ë°˜
    s2 = s.strip().strip("[]")
    parts = [p.strip().strip("'").strip('"') for p in re.split(r"[,/]", s2) if p.strip()]
    return [p for p in parts if p]

def load_food_db_simple(path: str = FOOD_DB_CSV) -> pd.DataFrame:
    df = pd.read_csv(path)
    for c in ["ì‹í’ˆ", "ë“±ê¸‰", "íƒœê·¸(ì˜ì–‘)"]:
        if c not in df.columns:
            df[c] = ""
    if "íƒœê·¸ë¦¬ìŠ¤íŠ¸" in df.columns:
        df["íƒœê·¸ë¦¬ìŠ¤íŠ¸"] = df["íƒœê·¸ë¦¬ìŠ¤íŠ¸"].apply(_parse_taglist_cell)
    else:
        df["íƒœê·¸ë¦¬ìŠ¤íŠ¸"] = df["íƒœê·¸(ì˜ì–‘)"].apply(_parse_tags_from_slash)
    return df[["ì‹í’ˆ", "ë“±ê¸‰", "íƒœê·¸(ì˜ì–‘)", "íƒœê·¸ë¦¬ìŠ¤íŠ¸"]]

def load_nutrient_dict_simple(path: str = NUTRIENT_DICT_CSV) -> Dict[str, str]:
    nd = pd.read_csv(path)
    for c in ["ì˜ì–‘ì†Œ", "í•œì¤„ì„¤ëª…"]:
        if c not in nd.columns:
            nd[c] = ""
    return {str(r["ì˜ì–‘ì†Œ"]).strip(): str(r["í•œì¤„ì„¤ëª…"]).strip() for _, r in nd.iterrows()}

_GRADE_ORDER = {"Avoid": 2, "Caution": 1, "Safe": 0}
def _worse_grade(g1: str, g2: str) -> str:
    return g1 if _GRADE_ORDER.get(g1, 0) >= _GRADE_ORDER.get(g2, 0) else g2
def _norm(s: str) -> str:
    return str(s or "").strip()


# ==================== íŒŒì„œ (ì½¤ë§ˆ/í”ŒëŸ¬ìŠ¤/ìˆ˜ëŸ‰) ====================
def split_items(text: str) -> List[str]:
    """ì‰¼í‘œ(,)ì™€ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¨¼ì € ë¶„ë¦¬í•œ ë’¤, ê° í† í°ì„ '+'ë¡œ ì¶”ê°€ ë¶„ë¦¬"""
    if not text:
        return []
    first = [p.strip() for p in re.split(r"[,|\n|(|)]+", text) if p.strip()]
    final = []
    for part in first:
        final += [q.strip() for q in part.split('+') if q.strip()]
    return final

def parse_qty(token: str) -> Tuple[str, float]:
    """í† í° ëì˜ ìˆ«ìë¥¼ ìˆ˜ëŸ‰ìœ¼ë¡œ íŒŒì‹±. ì—†ìœ¼ë©´ 1.0"""
    m = re.search(r"(.*?)(\d+(?:\.\d+)?)\s*$", token)
    if m:
        name = m.group(1).strip()
        qty = float(m.group(2))
        return name, qty
    return token.strip(), 1.0


# ================== ë§¤ì¹­/ë¶„ì„/ì¶”ì²œ ë¡œì§ ==================
def match_item_to_foods(item: str, df_food: pd.DataFrame) -> pd.DataFrame:
    """item(ì˜ˆ: 'ì†Œê³ ê¸° ë¯¸ì—­êµ­')ì— ëŒ€í•´ food_dbì˜ ì‹í’ˆëª…ì´ í¬í•¨ë˜ë©´ ë§¤ì¹­.
       ë°˜ëŒ€ë°©í–¥(í•­ëª©ëª…ì´ ë” ì§§ê³  DBê°€ ê¸¸ ë•Œ)ë„ í—ˆìš©."""
    it = _norm(item)
    hits = df_food[df_food["ì‹í’ˆ"].apply(lambda x: _norm(x) in it or it in _norm(x))].copy()
    hits = hits[hits["ì‹í’ˆ"].apply(lambda x: len(_norm(x)) >= 1)]
    return hits

def analyze_items_for_slot(input_text: str, slot: str, df_food: pd.DataFrame, nutrient_desc: Dict[str, str]):
    """ìŠ¬ë¡¯ ë‹¨ìœ„ ë¶„ì„ â†’ (items_df, nutrient_counts(dict), log_df, unmatched_names(list))"""
    raw_tokens = split_items(input_text)
    items = [parse_qty(tok) for tok in raw_tokens]  # [(name, qty), ...]

    per_item_rows = []
    nutrient_counts = defaultdict(float)
    log_rows = []
    unmatched_names = []

    for raw, qty in items:
        if not raw:
            continue
        matched = match_item_to_foods(raw, df_food)
        timestamp = datetime.now(TZ).isoformat(timespec="seconds")
        if matched.empty:
            per_item_rows.append({"ìŠ¬ë¡¯": slot, "ì…ë ¥í•­ëª©": raw, "ìˆ˜ëŸ‰": qty, "ë§¤ì¹­ì‹í’ˆ": "", "ë“±ê¸‰": "", "íƒœê·¸": ""})
            log_rows.append({
                "timestamp": timestamp,
                "date": datetime.now(TZ).date().isoformat(),
                "time": timestamp.split("T")[1],
                "slot": slot, "ì…ë ¥í•­ëª©": raw, "ìˆ˜ëŸ‰": qty, "ë§¤ì¹­ì‹í’ˆ": "", "ë“±ê¸‰": "", "íƒœê·¸": ""
            })
            unmatched_names.append(_norm(raw))
            continue

        agg_grade = "Safe"
        tag_union = []
        matched_names = []

        for _, r in matched.iterrows():
            name = _norm(r["ì‹í’ˆ"])
            grade = _norm(r["ë“±ê¸‰"]) or "Safe"
            tags = r.get("íƒœê·¸ë¦¬ìŠ¤íŠ¸", [])
            if not isinstance(tags, list):
                tags = _parse_taglist_cell(tags)
            if not tags:
                tags = _parse_tags_from_slash(r.get("íƒœê·¸(ì˜ì–‘)", ""))

            agg_grade = _worse_grade(agg_grade, grade)
            matched_names.append(name)
            for t in tags:
                if t:
                    tag_union.append(t)
                    nutrient_counts[t] += float(qty or 1.0)

        per_item_rows.append({
            "ìŠ¬ë¡¯": slot,
            "ì…ë ¥í•­ëª©": raw,
            "ìˆ˜ëŸ‰": qty,
            "ë§¤ì¹­ì‹í’ˆ": ", ".join(dict.fromkeys(matched_names)),
            "ë“±ê¸‰": agg_grade,
            "íƒœê·¸": ", ".join(dict.fromkeys(tag_union)),
        })
        log_rows.append({
            "timestamp": timestamp,
            "date": datetime.now(TZ).date().isoformat(),
            "time": timestamp.split("T")[1],
            "slot": slot,
            "ì…ë ¥í•­ëª©": raw,
            "ìˆ˜ëŸ‰": qty,
            "ë§¤ì¹­ì‹í’ˆ": ", ".join(dict.fromkeys(matched_names)),
            "ë“±ê¸‰": agg_grade,
            "íƒœê·¸": ", ".join(dict.fromkeys(tag_union)),
        })

    return (pd.DataFrame(per_item_rows), dict(nutrient_counts), pd.DataFrame(log_rows), unmatched_names)

def summarize_nutrients(nutrient_counts: Dict[str, float],
                        df_food: pd.DataFrame,
                        nutrient_desc: Dict[str, str],
                        threshold: int = 1) -> pd.DataFrame:
    all_tags = sorted({t for tlist in df_food["íƒœê·¸ë¦¬ìŠ¤íŠ¸"].apply(_parse_taglist_cell) for t in tlist})
    if not all_tags:
        return pd.DataFrame(columns=["ì˜ì–‘ì†Œ", "ìˆ˜ëŸ‰í•©", "ìƒíƒœ", "í•œì¤„ì„¤ëª…"])
    rows = []
    for tag in all_tags:
        cnt = float(nutrient_counts.get(tag, 0))
        rows.append({
            "ì˜ì–‘ì†Œ": tag,
            "ìˆ˜ëŸ‰í•©": cnt,
            "ìƒíƒœ": "ì¶©ì¡±" if cnt >= threshold else "ë¶€ì¡±",
            "í•œì¤„ì„¤ëª…": nutrient_desc.get(tag, "")
        })
    out = pd.DataFrame(rows)
    return out.sort_values(["ìƒíƒœ", "ìˆ˜ëŸ‰í•©", "ì˜ì–‘ì†Œ"], ascending=[True, False, True])


# ========= ê°œì„ ëœ ë‹¤ìŒ ì‹ì‚¬ ì œì•ˆ =========
def _tag_deficits(nutrient_counts: Dict[str, float],
                  tag_universe: List[str],
                  tag_targets: Dict[str, float] | None = None) -> Dict[str, float]:
    tag_targets = tag_targets or {}
    deficits = {}
    for t in tag_universe:
        target = float(tag_targets.get(t, 1.0))  # ê¸°ë³¸ ëª©í‘œ = 1
        cur = float(nutrient_counts.get(t, 0.0))
        lack = max(0.0, target - cur)
        if lack > 0:
            deficits[t] = lack
    return deficits

def _food_score(tags: list[str],
                deficits: Dict[str, float],
                grade: str,
                prefer_tags: set[str],
                avoid_tags: set[str],
                grade_weights: dict[str, float]) -> float:
    if not tags:
        return 0.0
    gain = sum(deficits.get(t, 0.0) for t in tags)
    gw = grade_weights.get(grade, 0.0)  # Safe=1.0, Caution=0.6, Avoid=0.0
    score = gain * gw
    if prefer_tags:
        score += 0.1 * sum(1.0 for t in tags if t in prefer_tags)
    if avoid_tags:
        score -= 0.2 * sum(1.0 for t in tags if t in avoid_tags)
    return score

def recommend_next_meal(nutrient_counts: Dict[str, float],
                        df_food: pd.DataFrame,
                        nutrient_desc: Dict[str, str],
                        *,
                        tag_targets: Dict[str, float] | None = None,
                        prefer_tags: list[str] | None = None,
                        avoid_tags: list[str] | None = None,
                        allowed_grades: tuple[str, ...] = ('Safe','Caution'),
                        grade_weights: dict[str, float] | None = None,
                        top_nutrients: int = 3,
                        per_food: int = 6,
                        max_items: int = 4):
    prefer_tags = set(prefer_tags or [])
    avoid_tags = set(avoid_tags or [])
    if grade_weights is None:
        grade_weights = {'Safe': 1.0, 'Caution': 0.6, 'Avoid': 0.0}

    tag_universe = sorted({t for lst in df_food["íƒœê·¸ë¦¬ìŠ¤íŠ¸"].apply(_parse_taglist_cell) for t in lst})
    if not tag_universe:
        return [], []

    deficits_all = _tag_deficits(nutrient_counts, tag_universe, tag_targets)
    if not deficits_all:
        return [], []

    lacking_sorted = sorted(deficits_all.items(), key=lambda x: x[1], reverse=True)
    focus_tags = {t for t, _ in lacking_sorted[:max(1, top_nutrients)]}

    cand = df_food[df_food["ë“±ê¸‰"].isin(allowed_grades)].copy()
    cand["íƒœê·¸ë¦¬ìŠ¤íŠ¸"] = cand["íƒœê·¸ë¦¬ìŠ¤íŠ¸"].apply(_parse_taglist_cell)
    cand = cand[cand["íƒœê·¸ë¦¬ìŠ¤íŠ¸"].apply(lambda lst: any(t in focus_tags for t in lst))]
    if cand.empty:
        return [], []

    cand["__score"] = cand.apply(
        lambda r: _food_score(r["íƒœê·¸ë¦¬ìŠ¤íŠ¸"], deficits_all, str(r["ë“±ê¸‰"]),
                              prefer_tags, avoid_tags, grade_weights),
        axis=1
    )
    cand = cand.sort_values("__score", ascending=False)

    suggestions = []
    for tag in sorted(focus_tags, key=lambda t: deficits_all[t], reverse=True):
        pool = cand[cand["íƒœê·¸ë¦¬ìŠ¤íŠ¸"].apply(lambda lst: tag in lst)]
        foods = pool["ì‹í’ˆ"].head(per_food).tolist()
        suggestions.append({"ë¶€ì¡±ì˜ì–‘ì†Œ": tag, "ì„¤ëª…": nutrient_desc.get(tag, ""), "ì¶”ì²œì‹í’ˆ": foods})

    remaining = deficits_all.copy()
    picked = []
    for _, row in cand.iterrows():
        if len(picked) >= max_items:
            break
        tags = row["íƒœê·¸ë¦¬ìŠ¤íŠ¸"]
        gain = sum(remaining.get(t, 0.0) for t in tags)
        if gain <= 0:
            continue
        picked.append(str(row["ì‹í’ˆ"]))
        for t in tags:
            if t in remaining:
                remaining[t] = max(0.0, remaining[t] - 1.0)

    return suggestions, picked[:max_items]


def append_unmatched_to_food_db(df_food: pd.DataFrame, unmatched_names: List[str]) -> pd.DataFrame:
    """ë§¤ì¹­ ì•ˆ ëœ ì‹í’ˆëª…ì„ ì‹í’ˆ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€(ë“±ê¸‰/íƒœê·¸ ë¹„ì›Œë‘ ). ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê±´ë„ˆëœ€."""
    to_add = []
    existing = set(df_food["ì‹í’ˆ"].astype(str).str.strip().tolist())
    for name in unmatched_names:
        if not name:
            continue
        if name in existing:
            continue
        to_add.append({"ì‹í’ˆ": name, "ë“±ê¸‰": "", "íƒœê·¸(ì˜ì–‘)": "", "íƒœê·¸ë¦¬ìŠ¤íŠ¸": []})
        existing.add(name)
    if to_add:
        df_new = pd.concat([df_food, pd.DataFrame(to_add)], ignore_index=True)
    else:
        df_new = df_food.copy()
    return df_new


# ==================== URL ìƒíƒœ ì €ì¥/ë³µì› ====================
def _b64_encode(obj: dict) -> str:
    raw = json.dumps(obj, ensure_ascii=False, separators=(",", ":")).encode("utf-8")
    comp = zlib.compress(raw, level=9)
    return base64.urlsafe_b64encode(comp).decode("ascii")

def _b64_decode(s: str) -> dict | None:
    try:
        comp = base64.urlsafe_b64decode(s.encode("ascii"))
        raw = zlib.decompress(comp)
        return json.loads(raw.decode("utf-8"))
    except Exception:
        return None

def _set_query_state(state_b64: str):
    try:
        st.query_params.update({"state": state_b64})  # Streamlit ìµœì‹ 
    except Exception:
        st.experimental_set_query_params(state=state_b64)  # êµ¬ë²„ì „

def _get_query_state() -> str | None:
    try:
        params = st.query_params  # Streamlit ìµœì‹ 
        return params.get("state", None)
    except Exception:
        params = st.experimental_get_query_params()  # êµ¬ë²„ì „
        vals = params.get("state")
        return vals[0] if vals else None

def save_state_to_url():
    """í˜„ì¬ ì…ë ¥/ê²°ê³¼ë¥¼ URL ì¿¼ë¦¬íŒŒë¼ë¯¸í„°(state=...)ë¡œ ì €ì¥."""
    data = {
        "inputs": st.session_state.get("inputs", {}),
        "threshold": st.session_state.get("threshold", 1),
        "export_flag": st.session_state.get("export_flag", True),
        "last_items_df": (
            st.session_state.last_items_df.to_dict(orient="split")
            if st.session_state.get("last_items_df", None) is not None else None
        ),
        "last_nutri_df": (
            st.session_state.last_nutri_df.to_dict(orient="split")
            if st.session_state.get("last_nutri_df", None) is not None else None
        ),
        "last_recs": st.session_state.get("last_recs", []),
        "last_combo": st.session_state.get("last_combo", []),
        "daily_date": st.session_state.get("daily_date", None),
    }
    _set_query_state(_b64_encode(data))

def load_state_from_url():
    """URL ì¿¼ë¦¬íŒŒë¼ë¯¸í„°(state)ì—ì„œ ìƒíƒœ ë³µì›."""
    state = _get_query_state()
    if not state:
        return
    decoded = _b64_decode(state)
    if not decoded:
        return
    if decoded.get("daily_date") and decoded["daily_date"] != st.session_state.get("daily_date"):
        return  # ë‚ ì§œ ë°”ë€Œë©´ ë³µì›í•˜ì§€ ì•ŠìŒ(ìì • ì´ˆê¸°í™” ìœ ì§€)

    st.session_state.inputs = decoded.get("inputs", {})
    st.session_state["threshold"] = decoded.get("threshold", 1)
    st.session_state["export_flag"] = decoded.get("export_flag", True)

    li = decoded.get("last_items_df")
    ln = decoded.get("last_nutri_df")
    st.session_state.last_items_df = pd.DataFrame(**li) if li else None
    st.session_state.last_nutri_df = pd.DataFrame(**ln) if ln else None
    st.session_state.last_recs = decoded.get("last_recs", [])
    st.session_state.last_combo = decoded.get("last_combo", [])


# ==================== Streamlit UI ====================
def main():
    # í˜ì´ì§€ ì„¤ì • & ì œëª©
    st.set_page_config(page_title="ìŠ¬ë¡¯ë³„ ì‹ë‹¨ ë¶„ì„ Â· ë‹¤ìŒ ì‹ì‚¬ ì œì•ˆ", page_icon="ğŸ¥—", layout="centered")
    st.title("ğŸ¥— ìŠ¬ë¡¯ë³„ ì‹ë‹¨ ë¶„ì„ Â· ë‹¤ìŒ ì‹ì‚¬ ì œì•ˆ")

    # âœ… ìì • ê¸°ì¤€ í•˜ë£¨ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    init_daily_state()
    remain = (next_midnight() - datetime.now(TZ))
    st.caption(f"í˜„ì¬ ì…ë ¥/ê²°ê³¼ëŠ” **ìì •ê¹Œì§€ ìë™ ë³´ì¡´**ë©ë‹ˆë‹¤. ë‚¨ì€ ì‹œê°„: ì•½ {remain.seconds//3600}ì‹œê°„ {remain.seconds%3600//60}ë¶„")

    # âœ… ìƒˆë¡œê³ ì¹¨/ìƒˆ íƒ­ ë³µì›: URLì— ì €ì¥ëœ ìƒíƒœ ë¡œë“œ
    load_state_from_url()

    # íŒŒì¼ ë¡œë”©
    with st.expander("ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •", expanded=False):
        food_path = st.text_input("food_db.csv ê²½ë¡œ", value=FOOD_DB_CSV)
        nutri_path = st.text_input("nutrient_dict.csv ê²½ë¡œ", value=NUTRIENT_DICT_CSV)
        load_btn = st.button("íŒŒì¼ ë‹¤ì‹œ ë¡œë“œ")
    if load_btn:
        st.experimental_rerun()

    # ì‹¤ì œ ë¡œë”©
    try:
        df_food = load_food_db_simple(food_path if 'food_path' in locals() else FOOD_DB_CSV)
        nutrient_desc = load_nutrient_dict_simple(nutri_path if 'nutri_path' in locals() else NUTRIENT_DICT_CSV)
    except Exception as e:
        st.error(f"CSV ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        st.stop()

    st.caption("ì…ë ¥ ì˜ˆ: ì†Œê³ ê¸° ë¯¸ì—­êµ­, ì°¹ìŒ€ë°¥, ì´ê°ê¹€ì¹˜1, ë¬´ìŒˆ, ìš°ë©”ë³´ì‹œ2, ë‹­ê³ ê¸°2, ë“¤ê¸°ë¦„, ì˜¬ë¦¬ë¸Œìœ  ì‚¬ê³¼+ì‹œë‚˜ëª¬ê°€ë£¨, ë¸”ë™ì»¤í”¼1")

    # ë‚ ì§œ(ê¸°ë¡ìš©)
    d = st.date_input("ê¸°ë¡ ë‚ ì§œ", value=date.today())

    # ìŠ¬ë¡¯ë³„ ì…ë ¥
    with st.container():
        for slot in SLOTS:
            val = st.text_area(
                slot, height=70, placeholder=f"{slot}ì— ë¨¹ì€ ê²ƒ ì…ë ¥",
                key=f"ta_{slot}",
                value=st.session_state.inputs.get(slot, "")
            )
            # âœ… ì‹¤ì‹œê°„ ë°˜ì˜
            if val != st.session_state.inputs.get(slot, ""):
                st.session_state.inputs[slot] = val
                # ì¦‰ì‹œ log.csv ì—…ë°ì´íŠ¸
                new_log = pd.DataFrame([{
                    "timestamp": datetime.now(TZ).isoformat(timespec="seconds"),
                    "date": today_str(),
                    "time": datetime.now(TZ).strftime("%H:%M:%S"),
                    "slot": slot,
                    "ì…ë ¥í•­ëª©": val,
                    "ìˆ˜ëŸ‰": "",
                    "ë§¤ì¹­ì‹í’ˆ": "",
                    "ë“±ê¸‰": "",
                    "íƒœê·¸": ""
                }])
                try:
                    old = pd.read_csv(LOG_CSV)
                    merged = pd.concat([old, new_log], ignore_index=True)
                except Exception:
                    merged = new_log
                merged.to_csv(LOG_CSV, index=False, encoding="utf-8-sig")
            st.session_state.inputs[slot] = val

    # ì˜µì…˜/ë²„íŠ¼
    c1, c2, c3 = st.columns([1, 1, 1])
    with c1:
        st.number_input(
            "ì¶©ì¡± ì„ê³„(ìˆ˜ëŸ‰í•©)",
            min_value=1, max_value=5,
            value=st.session_state.get("threshold", 1),
            step=1,
            key="threshold"
        )
    with c2:
        st.checkbox(
            "log.csv ì €ì¥",
            value=st.session_state.get("export_flag", True),
            key="export_flag"
        )
    with c3:
        analyze_clicked = st.button("ë¶„ì„í•˜ê¸°", type="primary", key="analyze_btn")

    # âœ… ì…ë ¥/ì˜µì…˜ ë³€ê²½ ì§í›„ì—ë„ URLì— ìƒíƒœ ì €ì¥ â†’ ë¶„ì„ ì „ ìƒˆë¡œê³ ì¹¨ì—ë„ ë³µì›ë¨
    save_state_to_url()

    # ===== ë¶„ì„ ì‹¤í–‰ =====
    if analyze_clicked:
        try:
            all_items_df_list = []
            total_counts = defaultdict(float)
            all_logs = []
            all_unmatched = []

            for slot in SLOTS:
                items_df, counts, log_df, unmatched = analyze_items_for_slot(
                    st.session_state.inputs.get(slot, ""), slot, df_food, nutrient_desc
                )
                if not items_df.empty:
                    items_df["ë‚ ì§œ"] = d.isoformat()
                if not log_df.empty:
                    log_df["date"] = d.isoformat()
                all_items_df_list.append(items_df)
                for k, v in counts.items():
                    total_counts[k] += float(v or 0)
                all_logs.append(log_df)
                all_unmatched += unmatched

            items_df_all = (
                pd.concat(all_items_df_list, ignore_index=True)
                if all_items_df_list else
                pd.DataFrame(columns=["ìŠ¬ë¡¯", "ì…ë ¥í•­ëª©", "ìˆ˜ëŸ‰", "ë§¤ì¹­ì‹í’ˆ", "ë“±ê¸‰", "íƒœê·¸", "ë‚ ì§œ"])
            )
            logs_all = (
                pd.concat(all_logs, ignore_index=True)
                if all_logs else
                pd.DataFrame(columns=["timestamp", "date", "time", "slot", "ì…ë ¥í•­ëª©", "ìˆ˜ëŸ‰", "ë§¤ì¹­ì‹í’ˆ", "ë“±ê¸‰", "íƒœê·¸"])
            )

            # âœ… ê²°ê³¼ ë³´ì¡´
            st.session_state.last_items_df = items_df_all
            st.session_state.last_nutri_df = summarize_nutrients(
                dict(total_counts), df_food, nutrient_desc, threshold=int(st.session_state["threshold"])
            )
            st.session_state.last_recs, st.session_state.last_combo = recommend_next_meal(
                dict(total_counts), df_food, nutrient_desc,
                # tag_targets={'ë‹¨ë°±ì§ˆ': 2, 'ì‹ì´ì„¬ìœ ': 2},
                # prefer_tags=['ì‹ì´ì„¬ìœ ','ë‹¨ë°±ì§ˆ'],
                # avoid_tags=['ë‹¹','íƒ„ìˆ˜í™”ë¬¼'],
                # allowed_grades=('Safe','Caution'),
                # max_items=4
            )

            # âœ… ë¶„ì„ ê²°ê³¼ê¹Œì§€ URLì— ì €ì¥ (ë¦¬í”„ë ˆì‹œ í›„ì—ë„ ê·¸ëŒ€ë¡œ í‘œì‹œ)
            save_state_to_url()

            # ===== log.csv ì €ì¥ & ë‹¤ìš´ë¡œë“œ =====
            if st.session_state["export_flag"] and not logs_all.empty:
                try:
                    try:
                        prev = pd.read_csv(LOG_CSV)
                        merged = pd.concat([prev, logs_all], ignore_index=True)
                    except Exception:
                        merged = logs_all.copy()
                    merged.to_csv(LOG_CSV, index=False, encoding="utf-8-sig")
                    st.success(f"'{LOG_CSV}' ì €ì¥ ì™„ë£Œ")
                    with open(LOG_CSV, "rb") as f:
                        st.download_button("â¬‡ï¸ log.csv ë‹¤ìš´ë¡œë“œ", data=f.read(), file_name="log.csv", mime="text/csv")
                except Exception as ex:
                    st.error(f"log.csv ì €ì¥/ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {ex}")

            # ===== food_db ì—…ë°ì´íŠ¸ (ë¯¸ë§¤ì¹­ ì¬ë£Œ ì¶”ê°€) =====
            df_food_updated = append_unmatched_to_food_db(df_food, all_unmatched)
            try:
                df_export = df_food_updated.copy()
                df_export["íƒœê·¸ë¦¬ìŠ¤íŠ¸"] = df_export["íƒœê·¸ë¦¬ìŠ¤íŠ¸"].apply(_parse_taglist_cell)
                df_export["íƒœê·¸(ì˜ì–‘)"] = df_export["íƒœê·¸ë¦¬ìŠ¤íŠ¸"].apply(lambda lst: "/".join(lst))
                df_export.to_csv(FOOD_DB_UPDATED_CSV, index=False, encoding="utf-8-sig")
                st.success("ë¯¸ë§¤ì¹­ ì¬ë£Œë¥¼ í¬í•¨í•œ 'food_db_updated.csv' ìƒì„± ì™„ë£Œ")
                with open(FOOD_DB_UPDATED_CSV, "rb") as f:
                    st.download_button("â¬‡ï¸ food_db_updated.csv ë‹¤ìš´ë¡œë“œ", data=f.read(), file_name="food_db_updated.csv", mime="text/csv")
            except Exception as ex:
                st.error(f"food_db ì—…ë°ì´íŠ¸/ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {ex}")

        except Exception as e:
            st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

    # ===== í™”ë©´ í‘œì‹œ =====
    st.markdown("### ğŸ± ìŠ¬ë¡¯ë³„ ë§¤ì¹­ ê²°ê³¼")
    if st.session_state.last_items_df is None or st.session_state.last_items_df.empty:
        st.info("ë§¤ì¹­ëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.dataframe(
            st.session_state.last_items_df[["ë‚ ì§œ", "ìŠ¬ë¡¯", "ì…ë ¥í•­ëª©", "ìˆ˜ëŸ‰", "ë§¤ì¹­ì‹í’ˆ", "ë“±ê¸‰", "íƒœê·¸"]],
            use_container_width=True,
            height=min(420, 36 * (len(st.session_state.last_items_df) + 1))
        )

    st.markdown("### ğŸ§­ ì˜ì–‘ íƒœê·¸ ìš”ì•½ (ì¶©ì¡±/ë¶€ì¡± + í•œì¤„ì„¤ëª…)")
    if st.session_state.last_nutri_df is None or st.session_state.last_nutri_df.empty:
        st.info("ì˜ì–‘ì†Œ ì‚¬ì „ ë˜ëŠ” íƒœê·¸ ì •ë³´ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        st.dataframe(
            st.session_state.last_nutri_df,
            use_container_width=True,
            height=min(420, 36 * (len(st.session_state.last_nutri_df) + 1))
        )

    st.markdown("### ğŸ½ ë‹¤ìŒ ì‹ì‚¬ ì œì•ˆ (ë¶€ì¡± ë³´ì™„ìš©)")
    if not st.session_state.last_recs:
        st.success("í•µì‹¬ ë¶€ì¡± ì˜ì–‘ì†Œê°€ ì—†ìŠµë‹ˆë‹¤. ê· í˜•ì´ ì˜ ë§ì•˜ì–´ìš”!")
    else:
        for r in st.session_state.last_recs:
            foods_text = ", ".join(r["ì¶”ì²œì‹í’ˆ"]) if r["ì¶”ì²œì‹í’ˆ"] else "(ì¶”ì²œ ì‹í’ˆ ì—†ìŒ)"
            st.write(f"- **{r['ë¶€ì¡±ì˜ì–‘ì†Œ']}**: {r['ì„¤ëª…']}")
            st.caption(f"  ì¶”ì²œ ì‹í’ˆ: {foods_text}")
        if st.session_state.last_combo:
            st.info("ê°„ë‹¨ ì¡°í•© ì œì•ˆ: " + " / ".join(st.session_state.last_combo[:4]))


if __name__ == "__main__":
    if st is None:
        print("This script requires Streamlit to run the UI. Install with: pip install streamlit")
        sys.exit(1)
    else:
        main()
