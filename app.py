#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
diet_analyzer.py (enhanced)
- ê° ì‹ì‚¬ ìŠ¬ë¡¯ë³„ ì»¨ë””ì…˜ ì…ë ¥ ê°€ëŠ¥
- log.csv / URL ìƒíƒœ / í™”ë©´ í‘œì‹œ ëª¨ë‘ ë°˜ì˜
- ë¬´í•œë³µì‚¬ í˜„ìƒ ë°©ì§€ (ìµœì‹  1ê±´ë§Œ ë³µì›)
- ë‹¤ìŒ ì‹ì‚¬ ì œì•ˆ (ì˜ì–‘ íƒœê·¸ + ì»¨ë””ì…˜ ê¸°ë°˜)
"""

from __future__ import annotations
import re, sys, ast, json, base64, zlib
from collections import defaultdict
from typing import List, Dict, Tuple, Any
from datetime import datetime, date, timedelta
from zoneinfo import ZoneInfo
import pandas as pd

try:
    import streamlit as st
except Exception:
    st = None  # allow import without Streamlit

# ====================== ì„¤ì • ======================
FOOD_DB_CSV = "food_db.csv"
NUTRIENT_DICT_CSV = "nutrient_dict.csv"
LOG_CSV = "log.csv"
FOOD_DB_UPDATED_CSV = "food_db_updated.csv"

SLOTS = ["ì•„ì¹¨", "ì•„ì¹¨ë³´ì¡°ì œ", "ì˜¤ì „ ê°„ì‹", "ì ì‹¬", "ì ì‹¬ë³´ì¡°ì œ",
         "ì˜¤í›„ ê°„ì‹", "ì €ë…", "ì €ë…ë³´ì¡°ì œ", "ì €ë… ê°„ì‹"]

TZ = ZoneInfo("Europe/Paris")

# ==================== ë‚ ì§œ/ìƒíƒœ ê´€ë¦¬ ====================
def today_str() -> str:
    return datetime.now(TZ).date().isoformat()

def next_midnight():
    now = datetime.now(TZ)
    return (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=TZ)

def init_daily_state():
    """ìì • ë‹¨ìœ„ë¡œ stateë¥¼ ìœ ì§€. ë‚ ì§œ ë°”ë€Œë©´ ìë™ ì´ˆê¸°í™”."""
    if "daily_date" not in st.session_state:
        st.session_state.daily_date = today_str()
    if st.session_state.daily_date != today_str():
        for k in ["inputs", "conditions", "last_items_df", "last_nutri_df", "last_recs", "last_combo"]:
            st.session_state.pop(k, None)
        st.session_state.daily_date = today_str()

    st.session_state.setdefault("inputs", {s: "" for s in SLOTS})
    st.session_state.setdefault("conditions", {s: "" for s in SLOTS})  # âœ… ì»¨ë””ì…˜ ì¶”ê°€
    st.session_state.setdefault("last_items_df", None)
    st.session_state.setdefault("last_nutri_df", None)
    st.session_state.setdefault("last_recs", [])
    st.session_state.setdefault("last_combo", [])
    st.session_state.setdefault("threshold", 1)
    st.session_state.setdefault("export_flag", True)

    # âœ… log.csvì—ì„œ ì˜¤ëŠ˜ ë‚ ì§œì˜ ìµœì‹  1ê±´ì”© ë³µì› (ë¬´í•œë³µì‚¬ ë°©ì§€)
    try:
        df_log = pd.read_csv(LOG_CSV)
        today_logs = df_log[df_log["date"] == today_str()]
        if not today_logs.empty:
            for slot in SLOTS:
                slot_logs = today_logs[today_logs["slot"] == slot]
                if not slot_logs.empty:
                    latest = slot_logs.sort_values("timestamp").tail(1).iloc[0]
                    st.session_state.inputs[slot] = str(latest.get("ì…ë ¥í•­ëª©", "") or "")
                    st.session_state.conditions[slot] = str(latest.get("ì»¨ë””ì…˜", "") or "")
            st.session_state.last_items_df = today_logs.rename(columns={"slot": "ìŠ¬ë¡¯", "date": "ë‚ ì§œ"})
    except FileNotFoundError:
        pass

# ==================== ìœ í‹¸ ====================
def _parse_tags_from_slash(cell) -> List[str]:
    if pd.isna(cell):
        return []
    return [t.strip() for t in str(cell).split('/') if t.strip()]

def _parse_taglist_cell(cell: Any) -> List[str]:
    if isinstance(cell, list):
        return [str(x).strip() for x in cell if str(x).strip()]
    s = "" if cell is None or (isinstance(cell, float) and pd.isna(cell)) else str(cell).strip()
    if not s or s == "[]":
        return []
    try:
        v = ast.literal_eval(s)
        if isinstance(v, list):
            return [str(x).strip() for x in v if str(x).strip()]
    except Exception:
        pass
    try:
        v = json.loads(s)
        if isinstance(v, list):
            return [str(x).strip() for x in v if str(x).strip()]
    except Exception:
        pass
    s2 = s.strip().strip("[]")
    parts = [p.strip().strip("'").strip('"') for p in re.split(r"[,/]", s2) if p.strip()]
    return [p for p in parts if p]

def load_food_db_simple(path: str = FOOD_DB_CSV) -> pd.DataFrame:
    df = pd.read_csv(path)
    for c in ["ì‹í’ˆ", "ë“±ê¸‰", "íƒœê·¸(ì˜ì–‘)"]:
        if c not in df.columns:
            df[c] = ""
    if "íƒœê·¸ë¦¬ìŠ¤íŠ¸" in df.columns:
        df["íƒœê·¸ë¦¬ìŠ¤íŠ¸"] = df["íƒœê·¸ë¦¬ìŠ¤íŠ¸"].apply(_parse_taglist_cell)
    else:
        df["íƒœê·¸ë¦¬ìŠ¤íŠ¸"] = df["íƒœê·¸(ì˜ì–‘)"].apply(_parse_tags_from_slash)
    return df[["ì‹í’ˆ", "ë“±ê¸‰", "íƒœê·¸(ì˜ì–‘)", "íƒœê·¸ë¦¬ìŠ¤íŠ¸"]]

def load_nutrient_dict_simple(path: str = NUTRIENT_DICT_CSV) -> Dict[str, str]:
    nd = pd.read_csv(path)
    for c in ["ì˜ì–‘ì†Œ", "í•œì¤„ì„¤ëª…"]:
        if c not in nd.columns:
            nd[c] = ""
    return {str(r["ì˜ì–‘ì†Œ"]).strip(): str(r["í•œì¤„ì„¤ëª…"]).strip() for _, r in nd.iterrows()}

_GRADE_ORDER = {"Avoid": 2, "Caution": 1, "Safe": 0}
def _worse_grade(g1: str, g2: str) -> str:
    return g1 if _GRADE_ORDER.get(g1, 0) >= _GRADE_ORDER.get(g2, 0) else g2
def _norm(s: str) -> str:
    return str(s or "").strip()

def split_items(text: str) -> List[str]:
    if not text:
        return []
    first = [p.strip() for p in re.split(r"[,|\n|(|)]+", text) if p.strip()]
    final = []
    for part in first:
        final += [q.strip() for q in part.split('+') if q.strip()]
    return final

def parse_qty(token: str) -> Tuple[str, float]:
    m = re.search(r"(.*?)(\d+(?:\.\d+)?)\s*$", token)
    if m:
        return m.group(1).strip(), float(m.group(2))
    return token.strip(), 1.0

# ================== ë¶„ì„ í•¨ìˆ˜ ==================
def match_item_to_foods(item: str, df_food: pd.DataFrame) -> pd.DataFrame:
    it = _norm(item)
    hits = df_food[df_food["ì‹í’ˆ"].apply(lambda x: _norm(x) in it or it in _norm(x))].copy()
    hits = hits[hits["ì‹í’ˆ"].apply(lambda x: len(_norm(x)) >= 1)]
    return hits

def analyze_items_for_slot(input_text: str, slot: str, df_food: pd.DataFrame,
                           nutrient_desc: Dict[str, str], condition: str = ""):
    raw_tokens = split_items(input_text)
    items = [parse_qty(tok) for tok in raw_tokens]

    per_item_rows, log_rows, unmatched_names = [], [], []
    nutrient_counts = defaultdict(float)

    for raw, qty in items:
        if not raw:
            continue
        matched = match_item_to_foods(raw, df_food)
        timestamp = datetime.now(TZ).isoformat(timespec="seconds")

        if matched.empty:
            per_item_rows.append({"ìŠ¬ë¡¯": slot, "ì…ë ¥í•­ëª©": raw, "ìˆ˜ëŸ‰": qty,
                                  "ë§¤ì¹­ì‹í’ˆ": "", "ë“±ê¸‰": "", "íƒœê·¸": "", "ì»¨ë””ì…˜": condition})
            log_rows.append({"timestamp": timestamp, "date": today_str(), "time": timestamp.split("T")[1],
                             "slot": slot, "ì…ë ¥í•­ëª©": raw, "ìˆ˜ëŸ‰": qty, "ë§¤ì¹­ì‹í’ˆ": "",
                             "ë“±ê¸‰": "", "íƒœê·¸": "", "ì»¨ë””ì…˜": condition})
            unmatched_names.append(_norm(raw))
            continue

        agg_grade, tag_union, matched_names = "Safe", [], []
        for _, r in matched.iterrows():
            name = _norm(r["ì‹í’ˆ"])
            grade = _norm(r["ë“±ê¸‰"]) or "Safe"
            tags = r.get("íƒœê·¸ë¦¬ìŠ¤íŠ¸", [])
            if not isinstance(tags, list):
                tags = _parse_taglist_cell(tags)
            if not tags:
                tags = _parse_tags_from_slash(r.get("íƒœê·¸(ì˜ì–‘)", ""))
            agg_grade = _worse_grade(agg_grade, grade)
            matched_names.append(name)
            for t in tags:
                if t:
                    tag_union.append(t)
                    nutrient_counts[t] += float(qty or 1.0)

        per_item_rows.append({"ìŠ¬ë¡¯": slot, "ì…ë ¥í•­ëª©": raw, "ìˆ˜ëŸ‰": qty,
                              "ë§¤ì¹­ì‹í’ˆ": ", ".join(dict.fromkeys(matched_names)),
                              "ë“±ê¸‰": agg_grade, "íƒœê·¸": ", ".join(dict.fromkeys(tag_union)),
                              "ì»¨ë””ì…˜": condition})
        log_rows.append({"timestamp": timestamp, "date": today_str(), "time": timestamp.split("T")[1],
                         "slot": slot, "ì…ë ¥í•­ëª©": raw, "ìˆ˜ëŸ‰": qty,
                         "ë§¤ì¹­ì‹í’ˆ": ", ".join(dict.fromkeys(matched_names)),
                         "ë“±ê¸‰": agg_grade, "íƒœê·¸": ", ".join(dict.fromkeys(tag_union)),
                         "ì»¨ë””ì…˜": condition})
    return (pd.DataFrame(per_item_rows), dict(nutrient_counts),
            pd.DataFrame(log_rows), unmatched_names)

# ==================== URL ìƒíƒœ ì €ì¥/ë³µì› ====================
def _b64_encode(obj: dict) -> str:
    raw = json.dumps(obj, ensure_ascii=False, separators=(",", ":")).encode("utf-8")
    comp = zlib.compress(raw, level=9)
    return base64.urlsafe_b64encode(comp).decode("ascii")

def _b64_decode(s: str) -> dict | None:
    try:
        comp = base64.urlsafe_b64decode(s.encode("ascii"))
        raw = zlib.decompress(comp)
        return json.loads(raw.decode("utf-8"))
    except Exception:
        return None

def save_state_to_url():
    data = {
        "inputs": st.session_state.get("inputs", {}),
        "conditions": st.session_state.get("conditions", {}),  # âœ… ì¶”ê°€
        "threshold": st.session_state.get("threshold", 1),
        "export_flag": st.session_state.get("export_flag", True),
        "daily_date": st.session_state.get("daily_date", None)
    }
    st.experimental_set_query_params(state=_b64_encode(data))

def load_state_from_url():
    params = st.experimental_get_query_params()
    if "state" not in params:
        return
    decoded = _b64_decode(params["state"][0])
    if not decoded:
        return
    st.session_state.inputs = decoded.get("inputs", {})
    st.session_state.conditions = decoded.get("conditions", {})
    st.session_state.threshold = decoded.get("threshold", 1)
    st.session_state.export_flag = decoded.get("export_flag", True)

# ==================== Streamlit UI ====================
def main():
    st.set_page_config(page_title="ìŠ¬ë¡¯ë³„ ì‹ë‹¨ ë¶„ì„ Â· ë‹¤ìŒ ì‹ì‚¬ ì œì•ˆ", page_icon="ğŸ¥—", layout="centered")
    st.title("ğŸ¥— ìŠ¬ë¡¯ë³„ ì‹ë‹¨ ë¶„ì„ Â· ë‹¤ìŒ ì‹ì‚¬ ì œì•ˆ")

    init_daily_state()
    remain = next_midnight() - datetime.now(TZ)
    st.caption(f"í˜„ì¬ ì…ë ¥/ê²°ê³¼ëŠ” ìì •ê¹Œì§€ ë³´ì¡´ë©ë‹ˆë‹¤. ë‚¨ì€ ì‹œê°„: {remain.seconds//3600}ì‹œê°„ {remain.seconds%3600//60}ë¶„")

    load_state_from_url()

    df_food = load_food_db_simple(FOOD_DB_CSV)
    nutrient_desc = load_nutrient_dict_simple(NUTRIENT_DICT_CSV)

    d = st.date_input("ê¸°ë¡ ë‚ ì§œ", value=date.today())

    for slot in SLOTS:
        val = st.text_area(slot, height=70, placeholder=f"{slot}ì— ë¨¹ì€ ê²ƒ ì…ë ¥",
                           key=f"ta_{slot}", value=st.session_state.inputs.get(slot, ""))
        st.session_state.inputs[slot] = val

        cond = st.text_input(f"{slot} ì»¨ë””ì…˜", placeholder="ì˜ˆ: ì–‘í˜¸ / í”¼ê³¤í•¨ / ë³µë¶€íŒ½ë§Œ",
                             key=f"cond_{slot}", value=st.session_state.conditions.get(slot, ""))
        st.session_state.conditions[slot] = cond

    c1, c2, c3 = st.columns([1, 1, 1])
    with c1:
        st.number_input("ì¶©ì¡± ì„ê³„(ìˆ˜ëŸ‰í•©)", min_value=1, max_value=5,
                        value=st.session_state.get("threshold", 1),
                        step=1, key="threshold")
    with c2:
        st.checkbox("log.csv ì €ì¥", value=st.session_state.get("export_flag", True), key="export_flag")
    with c3:
        analyze_clicked = st.button("ë¶„ì„í•˜ê¸°", type="primary", key="analyze_btn")

    save_state_to_url()

    if analyze_clicked:
        all_items, all_logs, total_counts, all_unmatched = [], [], defaultdict(float), []
        for slot in SLOTS:
            items_df, counts, log_df, unmatched = analyze_items_for_slot(
                st.session_state.inputs.get(slot, ""), slot, df_food, nutrient_desc,
                condition=st.session_state.conditions.get(slot, "")
            )
            if not items_df.empty:
                items_df["ë‚ ì§œ"] = d.isoformat()
            all_items.append(items_df)
            all_logs.append(log_df)
            all_unmatched += unmatched
            for k, v in counts.items():
                total_counts[k] += v

        items_df_all = pd.concat(all_items, ignore_index=True) if all_items else pd.DataFrame()
        logs_all = pd.concat(all_logs, ignore_index=True) if all_logs else pd.DataFrame()

        if st.session_state.export_flag and not logs_all.empty:
            try:
                try:
                    prev = pd.read_csv(LOG_CSV)
                    merged = pd.concat([prev, logs_all], ignore_index=True)
                except Exception:
                    merged = logs_all.copy()
                merged = merged.drop_duplicates(
                    subset=["date","slot","ì…ë ¥í•­ëª©","ë§¤ì¹­ì‹í’ˆ","ë“±ê¸‰","íƒœê·¸","ì»¨ë””ì…˜"], keep="last"
                )
                merged.to_csv(LOG_CSV, index=False, encoding="utf-8-sig")
                st.success("log.csv ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                st.error(f"log.csv ì €ì¥ ì˜¤ë¥˜: {e}")

        st.session_state.last_items_df = items_df_all

        # ================== ë‹¤ìŒ ì‹ì‚¬ ì œì•ˆ ==================
        st.markdown("### ğŸ½ ë‹¤ìŒ ì‹ì‚¬ ì œì•ˆ")

        # ì»¨ë””ì…˜ í‚¤ì›Œë“œ â†’ ê¶Œì¥ íƒœê·¸ ë§¤í•‘
        condition_map = {
            "í”¼ê³¤": {"tags": ["ì² ë¶„", "ë¹„íƒ€ë¯¼Bêµ°", "ë‹¨ë°±ì§ˆ"], "reason": "ì—ë„ˆì§€ ëŒ€ì‚¬ì™€ í”¼ë¡œ íšŒë³µ ì§€ì›"},
            "ë³µë¶€íŒ½ë§Œ": {"tags": ["ì‹ì´ì„¬ìœ ", "ì†Œí™”íš¨ì†Œ", "í”„ë¡œë°”ì´ì˜¤í‹±ìŠ¤"], "reason": "ì†Œí™” ê°œì„  ë° ì¥ë‚´ ê°€ìŠ¤ ì™„í™”"},
            "ì†ì“°ë¦¼": {"tags": ["ì €ì§€ë°©", "ì•Œì¹¼ë¦¬ì„±ì‹í’ˆ"], "reason": "ìœ„ì‚° ì¤‘í™” ë° ìê·¹ ì™„í™”"},
            "ë‘í†µ": {"tags": ["ë§ˆê·¸ë„¤ìŠ˜", "ìˆ˜ë¶„"], "reason": "ê¸´ì¥ ì™„í™” ë° ìˆ˜ë¶„ ë³´ì¶©"},
            "ë¶ˆë©´": {"tags": ["íŠ¸ë¦½í† íŒ", "ë§ˆê·¸ë„¤ìŠ˜"], "reason": "ìˆ˜ë©´ í˜¸ë¥´ëª¬ ë¶„ë¹„ ìœ ë„"},
            "ìŠ¤íŠ¸ë ˆìŠ¤": {"tags": ["ë¹„íƒ€ë¯¼C", "ë§ˆê·¸ë„¤ìŠ˜"], "reason": "ìŠ¤íŠ¸ë ˆìŠ¤ ì™„í™”ì™€ ì‹ ê²½ ì•ˆì •"},
            "í”¼ë¡œ": {"tags": ["ì² ë¶„", "ë¹„íƒ€ë¯¼Bêµ°", "ë‹¨ë°±ì§ˆ"], "reason": "ì²´ë ¥ íšŒë³µì— ë„ì›€"},
            "ë³€ë¹„": {"tags": ["ì‹ì´ì„¬ìœ ", "ìˆ˜ë¶„"], "reason": "ë°°ë³€ ê°œì„  ë° ì¥ìš´ë™ ì´‰ì§„"},
        }

        df_food_safe = df_food[df_food["ë“±ê¸‰"].fillna("Safe") == "Safe"]
        st.session_state.last_recs = []

        # ---- â‘  ì»¨ë””ì…˜ ê¸°ë°˜ ì œì•ˆ ----
        used_conditions = set(
            c for c in st.session_state.conditions.values()
            if isinstance(c, str) and c.strip()
        )

        cond_recs = []
        for cond in used_conditions:
            for key, v in condition_map.items():
                if key in cond:
                    tags = v["tags"]
                    reason = v["reason"]
                    rec_foods = df_food_safe[
                        df_food_safe["íƒœê·¸ë¦¬ìŠ¤íŠ¸"].apply(
                            lambda lst: (isinstance(lst, list) and any(t in lst for t in tags))
                        )
                    ]["ì‹í’ˆ"].unique().tolist()
                    rec_sample = ", ".join(rec_foods[:5]) if rec_foods else "ì¶”ì²œ ì‹í’ˆ ì—†ìŒ"
                    st.markdown(f"**ì»¨ë””ì…˜: {cond}** â†’ {reason}")
                    st.markdown(f"ğŸ‘‰ ì¶”ì²œ ì‹í’ˆ: {rec_sample}")
                    cond_recs.append({cond: rec_foods[:5]})
        if not cond_recs:
            st.info("íŠ¹ì • ì»¨ë””ì…˜ ê¸°ë°˜ ì œì•ˆ ì—†ìŒ")

        # ---- â‘¡ ì˜ì–‘ íƒœê·¸ ë¶€ì¡± ë³´ì™„ ----
        if not total_counts:
            st.info("ë¶„ì„ëœ ì˜ì–‘ íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            th = st.session_state.get("threshold", 1)
            low_tags = [t for t, v in total_counts.items() if v < th]

            if low_tags:
                st.write("---")
                st.write("**ë¶€ì¡±í•œ ì˜ì–‘ íƒœê·¸ ë³´ì™„ ì œì•ˆ**")
                for t in low_tags:
                    desc = nutrient_desc.get(t, "")
                    if desc:
                        st.write(f"**{t}** ({desc}) ë¶€ì¡±")
                    else:
                        st.write(f"**{t}** ë¶€ì¡±")
                    recs = df_food_safe[
                        df_food_safe["íƒœê·¸ë¦¬ìŠ¤íŠ¸"].apply(
                            lambda lst: (isinstance(lst, list) and t in lst)
                        )
                    ]["ì‹í’ˆ"].unique().tolist()
                    if recs:
                        sample = ", ".join(recs[:5])
                        st.markdown(f"ğŸ‘‰ ì¶”ì²œ ì‹í’ˆ: {sample}")
                        st.session_state.last_recs.append({t: recs[:5]})
                    else:
                        st.markdown(f"âš ï¸ `{t}` íƒœê·¸ë¥¼ ê°€ì§„ ì•ˆì „ ì‹í’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.success("í˜„ì¬ ì‹ë‹¨ì—ì„œ ì£¼ìš” ì˜ì–‘ íƒœê·¸ê°€ ì¶©ë¶„íˆ ì¶©ì¡±ë˜ì—ˆìŠµë‹ˆë‹¤ ğŸ‰")

        # ì „ì²´ ì¶”ì²œ ì €ì¥(ì»¨ë””ì…˜+ë¶€ì¡± íƒœê·¸)
        st.session_state.last_recs += cond_recs

    # ================== ë§¤ì¹­ ê²°ê³¼ í…Œì´ë¸” (í•­ìƒ í‘œì‹œ) ==================
    st.markdown("### ğŸ± ìŠ¬ë¡¯ë³„ ë§¤ì¹­ ê²°ê³¼")
    if st.session_state.last_items_df is None or st.session_state.last_items_df.empty:
        st.info("ë§¤ì¹­ëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        cols = ["ë‚ ì§œ","ìŠ¬ë¡¯","ì…ë ¥í•­ëª©","ìˆ˜ëŸ‰","ë§¤ì¹­ì‹í’ˆ","ë“±ê¸‰","íƒœê·¸"]
        if "ì»¨ë””ì…˜" in st.session_state.last_items_df.columns:
            cols.append("ì»¨ë””ì…˜")
        st.dataframe(st.session_state.last_items_df[cols],
                     use_container_width=True,
                     height=min(420, 36*(len(st.session_state.last_items_df)+1)))

if __name__ == "__main__":
    if st is None:
        print("This script requires Streamlit. Install with: pip install streamlit")
        sys.exit(1)
    main()